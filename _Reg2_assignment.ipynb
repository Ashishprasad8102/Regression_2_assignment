{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c0f4e-6579-4879-9065-b978d01b59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. R-squared (R²), also known as the coefficient of determination, is a statistical measure used to assess\n",
    "the goodness of fit of a linear regression model. It provides information about how well the independent\n",
    "variable(s) (predictors) explain the variation in the dependent variable (response) in a linear regression\n",
    "model. R-squared values range from 0 to 1, with higher values indicating a better fit of the model to the\n",
    "data.\n",
    "\n",
    "The formula to calculate R-squared is as follows:\n",
    "                                                   R squre =\n",
    "    \n",
    "It's important to note that a high R-squared does not necessarily mean that the model is a good predictor \n",
    "or that it has a causal relationship between the independent and dependent variables. It only indicates the\n",
    "goodness of fit of the linear regression model to the data.\n",
    "\n",
    "While R-squared is a valuable measure, it should be used in conjunction with other evaluation techniques and\n",
    "domain knowledge to assess the overall quality and validity of a linear regression model. Additionally, \n",
    "R-squared may not be appropriate for all types of data or models, so it's essential to consider the context \n",
    "and limitations of its interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a66097-05d7-472b-85fe-4ece62354f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Adjusted R-squared, often denoted as R²adj, is a modification of the regular R-squared (R²) in the context \n",
    "of linear regression. While regular R² tells you how well the regression model explains the variance in the\n",
    "dependent variable, adjusted R-squared takes into account the number of predictors (independent variables)\n",
    "in the model and adjusts the R-squared value accordingly. It is a measure that penalizes the inclusion of\n",
    "unnecessary predictors in the model.\n",
    "\n",
    "Here's how adjusted R-squared is calculated and how it differs from regular R-squared:\n",
    "\n",
    "Calculate the adjusted R-squared (R²adj):\n",
    "\n",
    "R²adj = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "\n",
    "R² is the regular R-squared.\n",
    "n is the number of observations (data points).\n",
    "k is the number of predictors (independent variables) in the model.\n",
    "Key differences between adjusted R-squared and regular R-squared:\n",
    "\n",
    "Adjustment for Model Complexity: Adjusted R-squared penalizes the inclusion of additional predictors that do not\n",
    "significantly improve the model's fit. It accounts for the number of predictors in the model, and as you add more\n",
    "predictors, it will decrease if those predictors do not contribute enough explanatory power. Regular R-squared, on\n",
    "the other hand, does not consider model complexity and can increase when you add more predictors, even if they are \n",
    "not useful.\n",
    "\n",
    "Interpretability: Adjusted R-squared provides a more conservative estimate of a model's goodness of fit, making \n",
    "it more interpretable when comparing models with different numbers of predictors. Regular R-squared can give\n",
    "overly optimistic estimates of model fit when more predictors are added.\n",
    "\n",
    "Model Selection: Adjusted R-squared can be used as a tool for model selection. It helps you choose the best \n",
    "subset of predictors by comparing models with different combinations of variables. You can select the model\n",
    "with the highest adjusted R-squared as long as it doesn't include unnecessary predictors.\n",
    "\n",
    "Bias Towards Simplicity: Adjusted R-squared encourages model simplicity. When you add predictors to a model,\n",
    "the adjusted R-squared will only increase if those predictors contribute significantly to explaining the variance \n",
    "in the dependent variable. This discourages overfitting, where a model fits the noise in the data rather than \n",
    "the underlying patterns.\n",
    "\n",
    "In summary, adjusted R-squared is a more useful metric than regular R-squared when you want to evaluate the\n",
    "goodness of fit of a regression model, especially in situations where you are dealing with multiple predictors.\n",
    "It helps strike a balance between model complexity and model performance, guiding you to build more parsimonious\n",
    "and interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b96035-29b1-4603-80f2-57bed538c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Adjusted R-squared is particularly useful when you are working with multiple regression models and want to\n",
    "assess the goodness of fit while considering the number of predictors in the model. It helps address the issue \n",
    "of overfitting by penalizing the inclusion of unnecessary predictors. Here are some situations in which adjusted\n",
    "R-squared is more appropriate to use:\n",
    "\n",
    "Model Comparison: Adjusted R-squared is valuable when you are comparing multiple regression models with different\n",
    "sets of predictors. It allows you to evaluate which model provides the best balance between goodness of fit and \n",
    "model simplicity. You should prefer the model with the highest adjusted R-squared, provided it doesn't include \n",
    "extraneous or irrelevant predictors.\n",
    "\n",
    "Feature Selection: When you have a large number of potential predictors and want to select a subset of them for\n",
    "your model, adjusted R-squared can guide you in the selection process. It helps you identify the most relevant\n",
    "predictors that contribute meaningfully to explaining the variance in the dependent variable.\n",
    "\n",
    "Avoiding Overfitting: Overfitting occurs when a model fits the noise in the data rather than the underlying \n",
    "patterns. Adjusted R-squared discourages overfitting by decreasing when unnecessary predictors are added to \n",
    "the model. It encourages a more parsimonious and interpretable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea1339-73be-4819-85d3-2dc01862026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.RMSE (Root Mean Square Error):\n",
    "\n",
    "RMSE is a widely used metric for regression analysis, and it measures the average magnitude of the errors between\n",
    "predicted and actual values.\n",
    "It is calculated by taking the square root of the mean of the squared differences between predicted (ŷ) and actual\n",
    "(Y) values:\n",
    "RMSE = √[Σ(Yi - ŷi)² / n]\n",
    "\n",
    "Where:\n",
    "\n",
    "Yi is the actual observed value.\n",
    "ŷi is the predicted value.\n",
    "n is the number of data points.\n",
    "RMSE has the same unit of measurement as the dependent variable (Y), which makes it interpretable in the same \n",
    "units. Lower RMSE values indicate better model performance.\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "MSE measures the average of the squared errors between predicted and actual values.\n",
    "It is calculated as the mean of the squared differences between predicted (ŷ) and actual (Y) values:\n",
    "MSE = Σ(Yi - ŷi)² / n\n",
    "\n",
    "MSE is also used to quantify the model's prediction accuracy, but it does not take the square root of the \n",
    "error terms. This means that it penalizes larger errors more heavily than smaller errors.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "MAE measures the average of the absolute differences between predicted and actual values.\n",
    "It is calculated as the mean of the absolute differences between predicted (ŷ) and actual (Y) values:\n",
    "MAE = Σ|Yi - ŷi| / n\n",
    "\n",
    "MAE is less sensitive to outliers compared to RMSE and MSE because it does not square the errors. It provides\n",
    "a measure of the average magnitude of errors in the same units as the dependent variable.\n",
    "\n",
    "In summary, these metrics serve the following purposes in the context of regression analysis:\n",
    "\n",
    "RMSE: Measures the average size of the errors between predicted and actual values. It provides a sense of how \n",
    "well the model's predictions match the observed data and is sensitive to large errors.\n",
    "\n",
    "MSE: Measures the average of the squared errors, giving more weight to larger errors. It is often used in\n",
    "optimization problems and when you want to quantify the overall error of the model.\n",
    "\n",
    "MAE: Measures the average absolute differences between predicted and actual values. It is less sensitive \n",
    "to outliers and provides a more straightforward interpretation of prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c31a8-8201-4620-bf3b-2f39d95d9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Advantages of RMSE:\n",
    "\n",
    "Sensitivity to Large Errors: RMSE penalizes larger errors more heavily due to the squaring of errors. This can be \n",
    "advantageous when you want to give higher importance to significant errors in your model evaluation.\n",
    "\n",
    "Differentiability: RMSE is differentiable, which makes it suitable for use in optimization algorithms when you need\n",
    "to minimize the error to train a model.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "\n",
    "Sensitivity to Outliers: RMSE is sensitive to outliers because it squares the errors. Outliers can disproportionately\n",
    "influence the RMSE and make it less robust in the presence of extreme data points.\n",
    "\n",
    "Lack of Unit Interpretability: RMSE has the same unit of measurement as the dependent variable, which can make it \n",
    "less interpretable, especially when the units are not easily understandable.\n",
    "\n",
    "Advantages of MSE:\n",
    "\n",
    "Mathematical Properties: MSE is mathematically convenient for optimization problems and statistical analysis. It\n",
    "is often used in situations where mathematical simplicity is a priority.\n",
    "\n",
    "Differentiability: Similar to RMSE, MSE is differentiable, making it suitable for optimization algorithms.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "\n",
    "Sensitivity to Outliers: Like RMSE, MSE is sensitive to outliers due to the squaring of errors. Outliers can have\n",
    "a strong impact on the overall value of MSE.\n",
    "\n",
    "Lack of Unit Interpretability: MSE lacks unit interpretability because it uses squared errors. This can make it \n",
    "challenging to explain the results to non-technical stakeholders.\n",
    "\n",
    "Advantages of MAE:\n",
    "\n",
    "Robustness to Outliers: MAE is less sensitive to outliers compared to RMSE and MSE because it uses absolute errors.\n",
    "This makes it a more robust metric when dealing with datasets that contain extreme values.\n",
    "\n",
    "Unit Interpretability: MAE has the same unit of measurement as the dependent variable, making it highly interpretable\n",
    "and easy to communicate to a non-technical audience.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "\n",
    "Less Sensitivity to Large Errors: MAE treats all errors equally, regardless of their magnitude. This can be a \n",
    "disadvantage when you want to give more weight to significant errors in your analysis.\n",
    "\n",
    "Not Differentiable at Zero: MAE is not differentiable at zero, which can make it less suitable for certain \n",
    "optimization algorithms that rely on derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38b033-d532-496a-97e8-1f9dbdc895e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Lasso regularization, short for \"Least Absolute Shrinkage and Selection Operator,\" is a technique used in linear\n",
    "regression and other regression-related machine learning algorithms to prevent overfitting and select a subset of\n",
    "important features from a larger set of predictors. Lasso differs from Ridge regularization in its approach and the\n",
    "type of penalty it applies.\n",
    "\n",
    "diffrent from ridge regularization\n",
    "L1 Penalty (Lasso): Lasso uses the L1 penalty, which encourages sparsity in the coefficient values. It effectively \n",
    "sets some coefficients to exactly zero, leading to feature selection. As a result, Lasso produces sparse models with\n",
    "only a subset of predictors retained.\n",
    "\n",
    "L2 Penalty (Ridge): Ridge regularization, on the other hand, uses the L2 penalty, which penalizes the square of the \n",
    "coefficients. While Ridge helps mitigate multicollinearity (correlation between predictors) and shrinks coefficients,\n",
    "it rarely sets coefficients to exactly zero. Ridge encourages all predictors to be included in the model, although \n",
    "their influence may be reduced.\n",
    "\n",
    "When to use:\n",
    "Feature Selection: When you have a large number of predictors and suspect that many of them may not be relevant to\n",
    "the problem, Lasso can automatically select the most important features, resulting in a simpler and more interpretable\n",
    "model.\n",
    "\n",
    "Sparse Models: When you want a sparse model that includes only a subset of predictors, making it easier to interpret\n",
    "and potentially reducing overfitting.\n",
    "\n",
    "When Predictors Are Sparse or Highly Correlated: Lasso can handle cases where there are only a few important predictors\n",
    "among many irrelevant ones, as well as situations with multicollinearity (correlation between predictors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392a8fe-6d73-4835-932c-caf48719d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.Regularized linear models are a class of machine learning algorithms that help prevent overfitting by introducing\n",
    "a penalty term into the model's optimization process. This penalty term discourages the model from fitting the \n",
    "training data too closely, which can lead to overfitting. Two commonly used types of regularization for linear\n",
    "models are Ridge regularization and Lasso regularization.\n",
    "\n",
    "Suppose you are building a linear regression model to predict housing prices based on various features, such as the \n",
    "number of bedrooms, square footage, and neighborhood. Without regularization, the model may try to fit the training\n",
    "data too closely, capturing noise and outliers. This can lead to overfitting, where the model performs well on the \n",
    "training data but poorly on new, unseen data.\n",
    "\n",
    "To prevent overfitting, you can apply Ridge or Lasso regularization to the linear regression model. For example, if \n",
    "you use Ridge regularization with a moderate value of α, the model will adjust the coefficients to be smaller,\n",
    "reducing the influence of any single feature and making the model less sensitive to small fluctuations in the training\n",
    "data. This regularization helps improve the model's generalization to new data, reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac5200-5648-4803-8fea-ec0d89258268",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Regularized linear models, such as Ridge and Lasso regression, are powerful techniques for addressing overfitting\n",
    "and feature selection in regression analysis. However, they are not always the best choice, as they come with certain \n",
    "limitations and considerations. Here are some limitations of regularized linear models and reasons why they may not \n",
    "always be the best choice for regression analysis:\n",
    "\n",
    "Limited Flexibility: Regularized linear models impose constraints on the coefficients by adding penalty terms to the \n",
    "cost function. This constraint can be limiting when the true relationship between predictors and the dependent variable \n",
    "is highly nonlinear. In such cases, a more flexible model, like a nonlinear regression model or a decision tree-based \n",
    "model, may be a better choice to capture complex relationships.\n",
    "\n",
    "Ineffectiveness for High-Dimensional Data: While Lasso regression is effective for feature selection by driving some \n",
    "coefficients to zero, it may not perform well with high-dimensional data where there are many predictors and potentially\n",
    "collinear variables. In such cases, Ridge regression or other techniques like dimensionality reduction \n",
    "(Principal Component Analysis) might be more appropriate.\n",
    "\n",
    "Model Bias: Regularized linear models tend to bias the coefficients toward zero, especially when strong regularization \n",
    "is applied. This bias can lead to underfitting if there are important predictors with nonzero coefficients that are\n",
    "incorrectly pushed towards zero. In some cases, you may want a model that allows for larger coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324914e9-d609-4f60-aaa0-3835b8816c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (74452565.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    9.The choice between RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) as the \"best\" metric depends on the\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "9.The choice between RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) as the \"best\" metric depends on the\n",
    "specific context of your problem and your goals. Both RMSE and MAE are common metrics used for evaluating the performance\n",
    "of regression models, and they measure different aspects of the model's accuracy.\n",
    "\n",
    "In summary, there is no universal answer to whether RMSE or MAE is \"best.\" The choice depends on your specific problem,\n",
    "your tolerance for large errors, the presence of outliers, and the goals of your model. It's also common to use \n",
    "multiple metrics together or to consider other factors when evaluating model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78faa680-0c14-4ec9-b898-22d4094e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.1 Penalty (Lasso): Lasso uses the L1 penalty, which encourages sparsity in the coefficient values. It effectively\n",
    "sets some coefficients to exactly zero, leading to feature selection. As a result, Lasso produces sparse models with \n",
    "only a subset of predictors retained.\n",
    "\n",
    "L2 Penalty (Ridge): Ridge regularization, on the other hand, uses the L2 penalty, which penalizes the square of the \n",
    "coefficients. While Ridge helps mitigate multicollinearity (correlation between predictors) and shrinks coefficients,\n",
    "it rarely sets coefficients to exactly zero. Ridge encourages all predictors to be included in the model, although\n",
    "their influence may be reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aea5e-83de-4f63-9d60-3d1fd6cf3b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
